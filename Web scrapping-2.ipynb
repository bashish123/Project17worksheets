{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name,experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " #import require libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 :- 1. first get the webpage https://www.naukri.com/\n",
    "url='https://www.naukri.com/'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove pop up\n",
    "remove_pop_up_btn=driver.find_element_by_id('block')\n",
    "remove_pop_up_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "#we called the search bar for job search by using class name\n",
    "search_job=driver.find_element_by_class_name('sugInp')\n",
    "#write on search bar of job search\n",
    "search_job.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#called the search bar for job search by using xpath\n",
    "search_location=driver.find_element_by_xpath('//input[@name=\"location\"]')\n",
    "#write on search bar of job location\n",
    "search_location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Then click the search button.\n",
    "\n",
    "#do click using class_name function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Then scrape the data for the first 10 jobs results we get.\n",
    "\n",
    "#extract all the job titles\n",
    "Job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 job_title we have to run for loop up to 10 count\n",
    "Job_titles=[]\n",
    "for i in Job_title[:10]:\n",
    "    Job_titles.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist / Data Analyst -Business Analyst',\n",
       " 'Junior Data Analyst',\n",
       " 'Junior Data Analyst',\n",
       " 'Network Design & Data Analyst',\n",
       " 'Data analysts',\n",
       " 'Data Analyst',\n",
       " 'Consultant - Data Analyst',\n",
       " 'Assistant Vice President - MIS & Reporting ( Business Data Analyst)',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the company_name\n",
    "\n",
    "company_name=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 company_name we have to run for loop up to 10 count\n",
    "com_name=[]\n",
    "for i in company_name[:10]:\n",
    "    com_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inflexion Analytix Private Limited',\n",
       " 'Simplilearn',\n",
       " 'Simplilearn',\n",
       " 'Dell International Services India Private Limited',\n",
       " 'IBM India Pvt. Limited',\n",
       " 'CARNATION INFOTECH PRIVATE LIMITED',\n",
       " 'Flipkart Internet Private Limited',\n",
       " 'INTERTRUSTVITEOS CORPORATE AND FUND SERVICES PVT. LTD.',\n",
       " 'Flipkart Internet Private Limited',\n",
       " 'Uber']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the  job_location\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 job_location we have to run for loop up to 10 count\n",
    "\n",
    "job_loc=[]\n",
    "for i in job_location[:10]:\n",
    "        job_loc.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hyderabad/Secunderabad, Chennai, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru(Sector 2 HSR Layout)',\n",
       " '(WFH during Covid)',\n",
       " 'Bangalore/Bengaluru(Sector 2 HSR Layout)',\n",
       " '(WFH during Covid)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Lucknow, Bangalore/Bengaluru',\n",
       " '(WFH during Covid)',\n",
       " 'Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the  experience_required\n",
    "\n",
    "experience_required=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 experience_required we have to run for loop up to 10 count\n",
    "\n",
    "exp_req=[]\n",
    "for i in experience_required[:10]:\n",
    "        exp_req.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-3 Yrs',\n",
       " '0-2 Yrs',\n",
       " '0-2 Yrs',\n",
       " '0-2 Yrs',\n",
       " '5-10 Yrs',\n",
       " '0-5 Yrs',\n",
       " '0-2 Yrs',\n",
       " '10-15 Yrs',\n",
       " '3-5 Yrs',\n",
       " '1-3 Yrs']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job-title</th>\n",
       "      <th>Job-location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Sector 2 HSR Layout)</td>\n",
       "      <td>Simplilearn</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>Simplilearn</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Network Design &amp; Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Sector 2 HSR Layout)</td>\n",
       "      <td>Dell International Services India Private Limited</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data analysts</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CARNATION INFOTECH PRIVATE LIMITED</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Consultant - Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Assistant Vice President - MIS &amp; Reporting ( B...</td>\n",
       "      <td>Lucknow, Bangalore/Bengaluru</td>\n",
       "      <td>INTERTRUSTVITEOS CORPORATE AND FUND SERVICES P...</td>\n",
       "      <td>10-15 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Uber</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job-title  \\\n",
       "0    Data Scientist / Data Analyst -Business Analyst   \n",
       "1                                Junior Data Analyst   \n",
       "2                                Junior Data Analyst   \n",
       "3                      Network Design & Data Analyst   \n",
       "4                                      Data analysts   \n",
       "5                                       Data Analyst   \n",
       "6                          Consultant - Data Analyst   \n",
       "7  Assistant Vice President - MIS & Reporting ( B...   \n",
       "8                                       Data Analyst   \n",
       "9                                       Data Analyst   \n",
       "\n",
       "                                        Job-location  \\\n",
       "0  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...   \n",
       "1           Bangalore/Bengaluru(Sector 2 HSR Layout)   \n",
       "2                                 (WFH during Covid)   \n",
       "3           Bangalore/Bengaluru(Sector 2 HSR Layout)   \n",
       "4                                 (WFH during Covid)   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                       Lucknow, Bangalore/Bengaluru   \n",
       "8                                 (WFH during Covid)   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                        Company_name Experience_required  \n",
       "0                 Inflexion Analytix Private Limited             0-3 Yrs  \n",
       "1                                        Simplilearn             0-2 Yrs  \n",
       "2                                        Simplilearn             0-2 Yrs  \n",
       "3  Dell International Services India Private Limited             0-2 Yrs  \n",
       "4                             IBM India Pvt. Limited            5-10 Yrs  \n",
       "5                 CARNATION INFOTECH PRIVATE LIMITED             0-5 Yrs  \n",
       "6                  Flipkart Internet Private Limited             0-2 Yrs  \n",
       "7  INTERTRUSTVITEOS CORPORATE AND FUND SERVICES P...           10-15 Yrs  \n",
       "8                  Flipkart Internet Private Limited             3-5 Yrs  \n",
       "9                                               Uber             1-3 Yrs  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Job-title']=Job_titles\n",
    "df['Job-location']=job_loc\n",
    "df['Company_name']=com_name\n",
    "df['Experience_required']=exp_req\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location,company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 :- 1. first get the webpage https://www.naukri.com/\n",
    "url='https://www.naukri.com/'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pop_up_btn=driver.find_element_by_id('block')\n",
    "remove_pop_up_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "#we called the search bar for job search by using class name\n",
    "search_job=driver.find_element_by_class_name('sugInp')\n",
    "#write on search bar of job search\n",
    "search_job.send_keys('Data Scientist')\n",
    "\n",
    "#called the search bar for job search by using xpath\n",
    "search_location=driver.find_element_by_xpath('//input[@name=\"location\"]')\n",
    "#write on search bar of job location\n",
    "search_location.send_keys('Bangalore')\n",
    "\n",
    "# 3. Then click the search button.\n",
    "\n",
    "#do click using class_name function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "#extract all the job titles\n",
    "Job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 job_title we have to run for loop up to 10 count\n",
    "Job_titles=[]\n",
    "for i in Job_title[:10]:\n",
    "        Job_titles.append(i.text)\n",
    "        \n",
    "#5.\n",
    "\n",
    "#extract all the company_name\n",
    "\n",
    "company_name=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 company_name we have to run for loop up to 10 count\n",
    "\n",
    "com_name=[]\n",
    "for i in company_name[:10]:\n",
    "        com_name.append(i.text)\n",
    "        \n",
    "#6.    \n",
    "#extract all the  job_location\n",
    "\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 job_location we have to run for loop up to 10 count\n",
    "\n",
    "job_loc=[]\n",
    "for i in job_location[:10]:\n",
    "        job_loc.append(i.text)\n",
    "\n",
    "#7.\n",
    "#extract all the  experience_required\n",
    "\n",
    "experience_required=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "#text inside the tage is extracted above\n",
    "#for extracting only first 10 experience_required we have to run for loop up to 10 count\n",
    "\n",
    "exp_req=[]\n",
    "for i in experience_required[:10]:\n",
    "        exp_req.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job-title</th>\n",
       "      <th>Job-location</th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>bd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>Oracle India Pvt. Ltd.</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...</td>\n",
       "      <td>inVentiv International Pharma Services Pvt. Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Gojek Tech</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GO-JEK India</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Job-title  \\\n",
       "0                            Senior Data Scientist   \n",
       "1                              Lead Data Scientist   \n",
       "2  Data Scientist / Data Analyst -Business Analyst   \n",
       "3                                   Data Scientist   \n",
       "4                                   Data Scientist   \n",
       "5               Data Scientist: Advanced Analytics   \n",
       "6                            Senior Data Scientist   \n",
       "7                                   Data Scientist   \n",
       "8                                 Data Scientist 2   \n",
       "9                                 Data Scientist 2   \n",
       "\n",
       "                                        Job-location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                         Noida, Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6  Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                       Company_name Experience_required  \n",
       "0                 Flipkart Internet Private Limited            5-10 Yrs  \n",
       "1                                                bd             2-7 Yrs  \n",
       "2                Inflexion Analytix Private Limited             0-3 Yrs  \n",
       "3                 Flipkart Internet Private Limited             0-5 Yrs  \n",
       "4                            Oracle India Pvt. Ltd.             5-9 Yrs  \n",
       "5                            IBM India Pvt. Limited            5-10 Yrs  \n",
       "6  inVentiv International Pharma Services Pvt. Ltd.             3-6 Yrs  \n",
       "7                                 Applied Materials            5-10 Yrs  \n",
       "8                                        Gojek Tech             4-8 Yrs  \n",
       "9                                      GO-JEK India            6-11 Yrs  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Job-title']=Job_titles\n",
    "df['Job-location']=job_loc\n",
    "df['Company_name']=com_name\n",
    "df['Experience_required']=exp_req\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Please note that you have to scrape full job description. For that you may have to open each job separately as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will scrap data of job description\n",
    "\n",
    "job_desc_url=[]\n",
    "url=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in url:\n",
    "    job_desc_url.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-323-1b461028419e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjob_desc_url\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mdesc1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div[@class=\"dang-inner-html\"]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdesc1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_alive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[0mstatuscode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     70\u001b[0m             return self.request_encode_body(method, url, fields=fields,\n\u001b[0;32m     71\u001b[0m                                             \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                                             **urlopen_kw)\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     def request_encode_url(self, method, url, fields=None, headers=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\request.py\u001b[0m in \u001b[0;36mrequest_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mextra_kw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mredirect_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mredirect\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_redirect_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "description=[]\n",
    "for i in job_desc_url[:10]:\n",
    "        driver.get(i)\n",
    "        desc1=driver.find_elements_by_xpath('//div[@class=\"dang-inner-html\"]')\n",
    "        for i in desc1:\n",
    "            description.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About the role\n",
      "A Senior Data Scientist in Flipkart is required to drive research and development of various Machine Learning and Statistical modules, keeping business goals and product planning at the front. The responsible person should be able to communicate and collaborate with multiple stakeholders representing various teams. The crux of the work is defined by developing innovative problem statements as well as end to end implementation of state of the art ML and statistical algorithms. The candidate is additionally expected to work proactively with a team of junior Data Scientists and guide them in various research projects with a view to publish their findings.\n",
      "What you’ll do\n",
      "Understand Business and product needs and use advanced modeling techniques to provide solutions to those in a time bound fashion.\n",
      "Develop research and hypothesis on problem statements around on-going work to provide additional value to the organisation.\n",
      "Keep the team updated on current trends in the area of ML and Statistics, thereby ensuring that state of the art solutions are provided for the business problems.\n",
      "Regular collaboration with various stakeholders from business or product to better understand the vision and gain insights which is to be leveraged for providing better solutions.\n",
      "\n",
      "What you’ll need:\n",
      "B.Tech in CS or Statistics with demonstrable experience of over 8 years through publications/deployed solutions/projects.\n",
      "M.Tech or PhD in CS or Statistics with demonstrable experience of over 5 years through publications/deployed solutions/projects.\n",
      "A deep understanding of applied statistical analysis and predictive modeling is desired.\n",
      "The candidate must have a thorough grasp of the theory and application of broad ML algorithms namely, but not limited to regression, SVM, Tree, Random Forests, Boosting, Neural Network, clustering, forecasting, deep learning, text analysis etc. It is not expected that the candidate has actually worked on all these modules.\n",
      "Strong proficiency in Python or R is necessary\n",
      "++++next++++\n",
      "Evaluating business requirements and developing compelling user stories in collaboration with business stakeholders across various functions and regions\n",
      "Collecting, interpreting, preparing and modelling data to provide actionable insights\n",
      "Prototyping advanced data analytics solutions for presentation to business partners and stakeholders\n",
      "Applying predictive analytics and machine learning methods and techniques appropriately to address business requirements\n",
      "Provide expertise to guide business stakeholders in identifying opportunities for the use of BD s data assets for the purpose of advanced data analytics\n",
      "Work closely with data engineering and DataOps peers to productionalize advanced data analytics use cases\n",
      "Qualifications\n",
      "Master s degree in STEM field or equivalent demonstrated work experience\n",
      "Experience with Python, R, Hadoop, HIVE, SPARK, Jupyter and related advanced analytics / machine learning libraries (Scikitlearn, Tensorflow, Keras, )\n",
      "Minimum 2 years of relevant work experience\n",
      "Ability to translate complex solutions to a non-technical audience (storytelling visualization)\n",
      "Expertise in machine learning, neural networks, clustering, classification, regression and other common advanced analytics methods and techniques\n",
      "Mature competency for critical thinking, problem solving, working with ambiguity, communications and collaboration\n",
      "Preferred\n",
      "Experience with technologies related to data preparation, processing optimization (e.g. Azure, HQL, SPARK SQL)\n",
      "Experience with data visualization tools (Power BI)\n",
      "Flexibility to accommodate meetings with international stakeholders in their respective time-zone\n",
      "++++next++++\n",
      "Job description\n",
      "\n",
      "Job Role: Data Scientist/Data Analyst /Business Analyst\n",
      "\n",
      "Location: Chennai/Bangalore/Hyderabad\n",
      "\n",
      "Greetings from CAIA - Center for Artificial Intelligence & Advanced Analytics\n",
      "43% of companies experienced a high deficit of skilled resources with Advanced Analytical skills and AI implementing capabilities in the year 2020. CAIA gives you a great opportunity to enter the world of future technologies and Innovations- Data Science, Analytics, AI, Data Visualization, and Cloud Computing.\n",
      "\n",
      "While 2020 was a year like no other, we are living in an interesting times where data is reshaping the world, and businesses are rapidly adopting technology to gain an edge over others. Hence, there's a substantial increase in demand for technology professionals who can implement systems in data science, machine learning, and AI in Tier 1 and Tier 2 organizations working closely with us.\n",
      "\n",
      "To help you build a sustainable career we would like you to utilize data, software and Analytical approaches in Data Science and AI to upskill and get recruited into an organization appreciating your skilling journey.\n",
      "Applications invited from all Freshers and experienced candidates (0-3 yrs) aspiring to make a career in Artificial Intelligence and Advanced Analytics and Data Science.\n",
      "If you wish to make a shift in your career or undergo a career transition, upskilling is essential since it allows you to learn more about the domain and acquire the required skills.\n",
      "\n",
      "Call to schedule interview Monday -Saturday from 10:00 am to 7 Pm\n",
      "\n",
      "Koodesh B- +91 73395 11107\n",
      "Manigandan B - +91 93444 57360\n",
      "\n",
      "Email :\n",
      "\n",
      "careerguidance.koodes@centerforaia.com\n",
      "manigandan@centerforaia.com\n",
      "\n",
      "What is needed from you?\n",
      "Freshers who wish to start their career in Analytics and AI and professionals who wish to\n",
      "upskill or change their domain to analytics and emerging technologies are free to apply.\n",
      "Educational background in any one of the following- BE/B.Tech, ME/M Tech, MSc, BSc/MSc Math's and Statistics, B Com, BCA, BSc CS, BSC IT, MSC IT, MCA\n",
      "Skills relating to Mathematics/Statistics.\n",
      "Natural passion towards numbers, business, coding, Analytics, and Artificial Intelligence, Machine Learning, visualization\n",
      "Good verbal and written communication skills\n",
      "Ability to understand domains in businesses across various sectors\n",
      "\n",
      "Selection procedure includes\n",
      "Aptitude Test & Communication Exam - Online / Offline\n",
      "SQL/Python test - Online / Offline\n",
      "Candidates who clear the above will have one-one discussions with our Career Guidance Manager for further evaluation and processing of your Resume.\n",
      "\n",
      "All the Shortlisted candidates will be eligible to continue the corporate training with CAIA\n",
      "What you can expect from us?\n",
      "\n",
      "You will get trained on the following modules for a period of 12-14 weeks:\n",
      "SQL & PLSQL\n",
      "Data Wrangling using Python\n",
      "Data Visualization Using Power-BI\n",
      "Statistics for Machine Learning\n",
      "Artificial Intelligence, Data Interpretation\n",
      "Supervised & Unsupervised Learning,\n",
      "NLP & Deep Learning\n",
      "Cloud Data Lake\n",
      "Business intelligence & Data Visualization\n",
      "Simulation Projects\n",
      "Expected Outcome?\n",
      "\n",
      "At the end of the Training you are expected to be well versed with the following:\n",
      "Analysis of large and complex data sets from multiple sources\n",
      "Development and evaluation of data analytics models, algorithms, and solutions\n",
      "Understanding/implementation of ML algorithms, performance tuning, and reporting\n",
      "Implementation of algorithms to mine targeted data and the ability to convert data into a business story\n",
      "Translation of business requirements into technical requirements; Data extraction, preparation, and transformation\n",
      "Identification, development, and implementation of statistical techniques and algorithms that address business challenges and adds value to the organization\n",
      "Requirement Analysis and communication of findings in the form of a meaningful story with the stakeholders\n",
      "Finding analytical solutions to abstract business issues.\n",
      "Apply objective analysis of facts before coming to a conclusion\n",
      "++++next++++\n",
      "About the team\n",
      "The Data Sciences team at Flipkart is on a mission to build systemic intelligence across Flipkart products and the overarching ecosystem. Being India’s largest online marketplace and the most used e-commerce app in India, places Flipkart in a unique position and gives this team a distinctive opportunity — to decipher the richest possible data about Indian consumers. Add the dimension of a vast product selection and a proliferating seller base to that and what you get is a multitude of disruptive possibilities.\n",
      "In a nutshell, the terabytes of daily data compounded in Flipkart’s data centres offer a dynamic mix of numerical, structured, unstructured, image- and audio-based statistics, all set to define shopping in the future.\n",
      "At Flipkart, the work of a Data Scientist involves collaboration with the engineering and product teams to ensure a holistic outcome at the product delivery. The flat functional structure within Flipkart engineering enables data scientists to focus on excellence and create a deep sense of ownership at every stage of work.\n",
      "If you aspire to redefine ‘state-of-the-art’ and create an impact on India’s online shopping landscape, Flipkart’s Data Science team can offer you the right podium to solve challenging real-world problems and take a giant leap in your career as a Data Scientist.\n",
      "About the role\n",
      "A Data Scientist in Flipkart is required to develop and implement ML or statistical models for the various projects formulated from business and product views. The responsible person should be able to communicate and collaborate with multiple stakeholders representing various teams to better understand the problem at hand. At a fundamental level, the responsible person should be able to dive deep into a problem statement and extract interesting insights as well as solutions. In addition to being a quick learner, a DS is expected to get involved in active research projects with a view to publish them.\n",
      "What you’ll do\n",
      "Understand Business and product needs and use ML or statistical techniques to provide solutions to those in a time bound fashion.\n",
      "Communicate and collaborate with business and product teams to have a better understanding of the project so as to be able to drive it within the DS team.\n",
      "Get involved in in-depth exploration of solutions using various methods and extract statistical insights from data as well as models, which are to be shared with business and product teams.\n",
      "Active participation in working with new methods and learning new technologies, both in the area of data science and data engineering.\n",
      "What you’ll need:\n",
      "B.Tech or M.Tech in CS or Statistics with experience ranging from 0 to 5 years through publications/deployed solutions/projects.\n",
      "Deep understanding of the algorithms (theory and application) that they have worked on.\n",
      "Good grasp on the theory and practise of basic statistical models such as regression or clustering and general ML algorithms such as tree, Random Forests, SVM, Boosting, Neural Networks etc. It is not expected that the candidate has actually worked on all these modules.\n",
      "Strong proficiency in Python or R is necessary.\n",
      "Skills Required :\n",
      "Understanding of text representation techniques\n",
      "Desirable Skills :\n",
      "Familiarity with Big Data frameworks – Spark, Hadoop\n",
      "++++next++++\n",
      "About the Role\n",
      "  They say no man is an island - a notion that holds particularly true for this role. As a Data Scientist, you ll be an instrumental cog in the Marketplace wheel of Gojek that directly impacts GoFoods revenue and user experience.\n",
      "   Your main objective will be to utilize various quantitative techniques such as Machine Learning, Optimization, Simulation, and Bayesian Techniques to drive asymmetric values for our business at Gojek. The folks over at the Data Science Platform team will be your companions during this ride, as they help to bring your models to production. Best yet, you ll have the opportunity to flex and hone in on your skills in ideation, research, and building prototypes!\n",
      "  What You Will Do\n",
      "Design and develop various machine learning solutions for improving search relevance\n",
      "Own the Data Science model end-to-end from data collection to model building to monitoring the model in production\n",
      "Along with Product Managers, own the business outcomes/metrics which the data science model/algorithm drives\n",
      "Identify ways to better leverage our content and improve its quality and attributes, to improve the overall search experience\n",
      "What You Will Need\n",
      "At least 5 years of experience in building machine learning models for product applications\n",
      "Must have the ability to understand business concerns and formulate them as technical problems that can be solved using data and math/stats/ML\n",
      "Familiarity with essential data science libraries, including Pandas, Numpy, Scipy, Scikit-Learn\n",
      "Experience building scalable ML solutions to NLP problems\n",
      "Familiarity with ML infrastructure needs and best practices\n",
      "Experience working with cross-functional teams including product, design, engineering, mobile to deliver product outcomes using data science\n",
      " \n",
      "++++next++++\n",
      "  About the Role\n",
      "  They say no man is an island - a notion that holds particularly true for this role. As a Data Scientist, you ll be an instrumental cog in the Marketplace wheel of Gojek that directly impacts GoFoods revenue and user experience. Your main objective will be to utilize various quantitative techniques such as Machine Learning, Optimization, Simulation, and Bayesian Techniques to drive asymmetric values for our business at Gojek.\n",
      "   The folks over at the Data Science Platform team will be your companions during this ride, as they help to bring your models to production. Best yet, you ll have the opportunity to flex and hone in on your skills in ideation, research, and building prototypes!\n",
      "  What You Will Do\n",
      "Design and develop various machine learning solutions for improving search relevance\n",
      "Own the Data Science model end-to-end from data collection to model building to monitoring the model in production\n",
      "Along with Product Managers, own the business outcomes/metrics which the data science model/algorithm drives\n",
      "Identify ways to better leverage our content and improve its quality and attributes, to improve the overall search experience\n",
      "What You Will Need\n",
      "At least 5 years of experience in building machine learning models for product applications\n",
      "Must have the ability to understand business concerns and formulate them as technical problems that can be solved using data and math/stats/ML\n",
      "Familiarity with essential data science libraries, including Pandas, Numpy, Scipy, Scikit-Learn\n",
      "Experience building scalable ML solutions to NLP problems\n",
      "Familiarity with ML infrastructure needs and best practices\n",
      "Experience working with cross-functional teams including product, design, engineering, mobile to deliver product outcomes using data science\n",
      "++++next++++\n"
     ]
    }
   ],
   "source": [
    "description\n",
    "for i in description:\n",
    "    print(i)\n",
    "    print(\"++++next++++\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 :- 1. first get the webpage https://www.naukri.com/\n",
    "url='https://www.naukri.com/'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pop_up_btn=driver.find_element_by_id('block')\n",
    "remove_pop_up_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_role=driver.find_element_by_xpath('//input[@name=\"keyword\"]')\n",
    "job_role.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do click using class_name function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do click on location check_box\n",
    "Location_box=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[1]/div[2]/div[3]')\n",
    "Location_box.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do click on salary check_box\n",
    "salary_box=driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]')\n",
    "salary_box.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get text data from web page by using xpath\n",
    "job_title=[] \n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "JT=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "JL=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "CN=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "ER=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate from all data at a time to append text in list\n",
    "c=0\n",
    "for (i,j,k,l) in itertools.zip_longest(JT[:10],JL[:10],CN[:10],ER[:10]):\n",
    "        job_title.append(i.text)\n",
    "        job_location.append(j.text)\n",
    "        company_name.append(k.text)\n",
    "        experience_required.append(l.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Eequired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist/Data Analyst /Business Analyst</td>\n",
       "      <td>Pune, Delhi / NCR, Mumbai (All Areas)</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...</td>\n",
       "      <td>inVentiv International Pharma Services Pvt. Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Team Leader Operations/Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Optimint Solutions Pvt. Ltd.</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>Noida, Greater Noida, Delhi / NCR</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist (Early Joiner)</td>\n",
       "      <td>Noida(Sector-59 Noida)</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Scientist - Machine Learning/ NLP</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>TalPro</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...</td>\n",
       "      <td>recruitment advisory line</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Scientist || Python || C2H</td>\n",
       "      <td>Noida, Kolkata, Gurgaon/Gurugram, Bangalore/Be...</td>\n",
       "      <td>Growel Softech Pvt. Ltd.</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data Scientist || Python || C2H</td>\n",
       "      <td>Noida, Kolkata, Gurgaon/Gurugram, Bangalore/Be...</td>\n",
       "      <td>Growel Softech Pvt. Ltd.</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Advanced Analytics -Data Scientist</td>\n",
       "      <td>New Delhi, Hyderabad/Secunderabad</td>\n",
       "      <td>ERM Placement Services (P) Ltd.</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0      Data Scientist/Data Analyst /Business Analyst   \n",
       "1                              Senior Data Scientist   \n",
       "2              Team Leader Operations/Data Scientist   \n",
       "3  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "4                      Data Scientist (Early Joiner)   \n",
       "5             Data Scientist - Machine Learning/ NLP   \n",
       "6                                     Data Scientist   \n",
       "7                    Data Scientist || Python || C2H   \n",
       "8                    Data Scientist || Python || C2H   \n",
       "9                 Advanced Analytics -Data Scientist   \n",
       "\n",
       "                                        Job Location  \\\n",
       "0              Pune, Delhi / NCR, Mumbai (All Areas)   \n",
       "1  Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...   \n",
       "2                      Gurgaon/Gurugram, Delhi / NCR   \n",
       "3                  Noida, Greater Noida, Delhi / NCR   \n",
       "4                             Noida(Sector-59 Noida)   \n",
       "5                                   Gurgaon/Gurugram   \n",
       "6  Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...   \n",
       "7  Noida, Kolkata, Gurgaon/Gurugram, Bangalore/Be...   \n",
       "8  Noida, Kolkata, Gurgaon/Gurugram, Bangalore/Be...   \n",
       "9                  New Delhi, Hyderabad/Secunderabad   \n",
       "\n",
       "                                       Company Name Experience Eequired  \n",
       "0                Inflexion Analytix Private Limited             0-3 Yrs  \n",
       "1  inVentiv International Pharma Services Pvt. Ltd.             3-6 Yrs  \n",
       "2                      Optimint Solutions Pvt. Ltd.             2-5 Yrs  \n",
       "3                         GABA Consultancy services             0-0 Yrs  \n",
       "4                      R Systems International Ltd.             4-8 Yrs  \n",
       "5                                            TalPro             2-6 Yrs  \n",
       "6                         recruitment advisory line             3-6 Yrs  \n",
       "7                          Growel Softech Pvt. Ltd.             4-6 Yrs  \n",
       "8                          Growel Softech Pvt. Ltd.             4-6 Yrs  \n",
       "9                   ERM Placement Services (P) Ltd.             3-7 Yrs  "
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe \n",
    "df=pd.DataFrame()\n",
    "df['Job Title']=job_title\n",
    "df['Job Location']=job_location\n",
    "df['Company Name']=company_name\n",
    "df['Experience Eequired']=experience_required\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url='https://www.glassdoor.co.in/index.htm'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do click on sign-in button\n",
    "sign_in=driver.find_element_by_xpath('//*[@id=\"SiteNav\"]/nav/div[1]/div[1]/button')\n",
    "sign_in.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Username\n",
    "entr_email=driver.find_element_by_id('userEmail')\n",
    "entr_email.send_keys('***Enter E-mail id***')\n",
    "#Enter Password\n",
    "entr_pass=driver.find_element_by_id('userPassword')\n",
    "entr_pass.send_keys('***Enter Password***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do click on sign-in button\n",
    "click_sign_in=driver.find_element_by_xpath('//*[@id=\"LoginModal\"]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[3]/div[1]/button')\n",
    "click_sign_in.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do click on search button\n",
    "click_search=driver.find_element_by_xpath('//*[@id=\"SearchForm\"]/div[1]')\n",
    "click_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Job title\n",
    "enter_title=driver.find_element_by_id('scKeyword')\n",
    "enter_title.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Job location\n",
    "location=driver.find_element_by_id('scLocation')\n",
    "location.send_keys('noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do click on search button\n",
    "click_search=driver.find_element_by_xpath('//button[@class=\"pl-0 pr-xsm SearchStyles__searchKeywordSubmit\"]')\n",
    "click_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list for every page link\n",
    "#then iterate it and fetch required data\n",
    "ls=[]\n",
    "nxt=driver.find_elements_by_xpath('//div[@class=\"d-flex flex-column css-x75kgh e1rrn5ka3\"]//a')\n",
    "for i in nxt:\n",
    "    ls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list\n",
    "company_name=[]\n",
    "No_of_days_ago=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=driver.find_elements_by_xpath('//div[@class=\"d-flex align-items-end pl-std css-mi55ob\"]')\n",
    "for i in d[:10]:\n",
    "    No_of_days_ago.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ls[:10]:\n",
    "    driver.get(i)\n",
    "    CN=driver.find_elements_by_xpath('//div[@class=\"css-16nw49e e11nt52q1\"]')\n",
    "    for (j) in CN:\n",
    "        company_name.append(j.text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bechtel4.0★',\n",
       " 'Bechtel4.0★',\n",
       " 'Ericsson4.1★',\n",
       " 'MobiKwik4.0★',\n",
       " 'Lantern Digital Services3.5★',\n",
       " 'Crowe3.8★',\n",
       " 'Novo4.2★',\n",
       " 'Siemens Technology and Services Private Limited4.1★',\n",
       " 'Biz2Credit Inc3.9★',\n",
       " 'Priority Vendor3.7★']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate Company name and rating\n",
    "c=[]\n",
    "Rating=[]\n",
    "for i in company_name:\n",
    "    if i[-1] =='★':\n",
    "        c.append(i[:-4])\n",
    "        Rating.append(i[-4:])\n",
    "    else:\n",
    "        c.append(i)\n",
    "        Rating.append('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Posted Duration</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bechtel</td>\n",
       "      <td>13d</td>\n",
       "      <td>4.0★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Bechtel</td>\n",
       "      <td>2d</td>\n",
       "      <td>4.0★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ericsson</td>\n",
       "      <td>6d</td>\n",
       "      <td>4.1★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MobiKwik</td>\n",
       "      <td>26d</td>\n",
       "      <td>4.0★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lantern Digital Services</td>\n",
       "      <td>4d</td>\n",
       "      <td>3.5★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Crowe</td>\n",
       "      <td>11d</td>\n",
       "      <td>3.8★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Novo</td>\n",
       "      <td>8d</td>\n",
       "      <td>4.2★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Siemens Technology and Services Private Limited</td>\n",
       "      <td>23d</td>\n",
       "      <td>4.1★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>19d</td>\n",
       "      <td>3.9★</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Priority Vendor</td>\n",
       "      <td>10d</td>\n",
       "      <td>3.7★</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Company Name Posted Duration Rating\n",
       "0                                          Bechtel             13d   4.0★\n",
       "1                                          Bechtel              2d   4.0★\n",
       "2                                         Ericsson              6d   4.1★\n",
       "3                                         MobiKwik             26d   4.0★\n",
       "4                         Lantern Digital Services              4d   3.5★\n",
       "5                                            Crowe             11d   3.8★\n",
       "6                                             Novo              8d   4.2★\n",
       "7  Siemens Technology and Services Private Limited             23d   4.1★\n",
       "8                                   Biz2Credit Inc             19d   3.9★\n",
       "9                                  Priority Vendor             10d   3.7★"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe \n",
    "df=pd.DataFrame()\n",
    "df['Company Name']=c\n",
    "df['Posted Duration']=No_of_days_ago[:10]\n",
    "df['Rating']=Rating\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url='https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Job title\n",
    "enter_title=driver.find_element_by_id('KeywordSearch')\n",
    "enter_title.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter Job location\n",
    "location=driver.find_element_by_id('LocationSearch')\n",
    "location.send_keys('noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do click on search button\n",
    "click_search=driver.find_element_by_xpath('//button[@class=\"gd-btn-mkt\"]')\n",
    "click_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists of required things\n",
    "company_name=[]\n",
    "Avg=[]\n",
    "Mix=[]\n",
    "salaries=[]\n",
    "minimum=[]\n",
    "maximum=[]\n",
    "Rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rat=driver.find_elements_by_xpath('//span[@class=\"m-0 css-kyx745\"]')\n",
    "for i in rat[:10]:\n",
    "    Rating.append(i.text)\n",
    "len(Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix=driver.find_elements_by_xpath('//span[@class=\"d-block d-lg-none m-0 css-1b6bxoo\"]')\n",
    "for i in mix:\n",
    "    Mix.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Range: ₹6L - ₹27L',\n",
       " 'Range: ₹3L - ₹13L',\n",
       " 'Range: ₹6L - ₹22L',\n",
       " 'Range: ₹5L - ₹1Cr',\n",
       " 'Range: ₹4L - ₹16L',\n",
       " 'Range: ₹8L - ₹15L',\n",
       " 'Range: ₹5L - ₹15L',\n",
       " 'Range: ₹6L - ₹15L',\n",
       " 'Range: ₹8L - ₹20L',\n",
       " 'Range: ₹4L - ₹22L',\n",
       " 'Range: ₹2L - ₹18L',\n",
       " 'Range: ₹6L - ₹17L',\n",
       " 'Range: ₹8L - ₹13L',\n",
       " 'Range: ₹10L - ₹19L',\n",
       " 'Range: ₹4L - ₹16L',\n",
       " 'Range: ₹8L - ₹20L',\n",
       " 'Range: ₹12T - ₹63T',\n",
       " 'Range: ₹4L - ₹17L',\n",
       " 'Range: ₹8L - ₹30L',\n",
       " 'Range: ₹9L - ₹15L']"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "#split min and max\n",
    "for i in Mix[:10]:\n",
    "    minimum.append(i[6:9])\n",
    "    maximum.append(i[12:])\n",
    "print(len(minimum),len(maximum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹9,00,000 /yr',\n",
       " '₹6,15,289 /yr',\n",
       " '₹11,63,336 /yr',\n",
       " '₹12,18,244 /yr',\n",
       " '₹7,39,238 /yr',\n",
       " '₹13,00,000 /yr',\n",
       " '₹8,63,750 /yr',\n",
       " '₹11,10,000 /yr',\n",
       " '₹14,23,677 /yr',\n",
       " '₹13,28,697 /yr',\n",
       " '₹11,42,356 /yr',\n",
       " '₹12,09,040 /yr',\n",
       " '₹10,09,021 /yr',\n",
       " '₹12,58,227 /yr',\n",
       " '₹9,92,632 /yr',\n",
       " '₹10,55,478 /yr',\n",
       " '₹34,157 /mo',\n",
       " '₹10,54,402 /yr',\n",
       " '₹18,40,134 /yr',\n",
       " '₹10,00,000 /yr']"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sal=driver.find_elements_by_xpath('//div[@class=\"col-12 col-lg-4 px-lg-0 d-flex align-items-baseline\"]')\n",
    "for i in sal:\n",
    "    Avg.append(i.text.replace('\\n',''))\n",
    "Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18 salaries',\n",
       " '17 salaries',\n",
       " '15 salaries',\n",
       " '15 salaries',\n",
       " '14 salaries',\n",
       " '13 salaries',\n",
       " '10 salaries',\n",
       " '9 salaries',\n",
       " '9 salaries',\n",
       " '9 salaries']"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa=driver.find_elements_by_xpath('//span[@class=\"m-0 css-1b6bxoo\"]')\n",
    "for i in sa:\n",
    "     salaries.append(i.text)\n",
    "S=salaries[2:12]\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p=salaries[2:12]\n",
    "# for i in p:\n",
    "#     salar.append(i[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN=driver.find_elements_by_xpath('//a[@class=\"css-f3vw95 e1aj7ssy3\"]')\n",
    "for i in CN:\n",
    "    company_name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Mini. Salary</th>\n",
       "      <th>Max. Salary</th>\n",
       "      <th>Salarise</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>IBM</td>\n",
       "      <td>₹9,00,000 /yr</td>\n",
       "      <td>₹6</td>\n",
       "      <td>₹27L</td>\n",
       "      <td>18 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>₹6,15,289 /yr</td>\n",
       "      <td>₹3</td>\n",
       "      <td>₹13L</td>\n",
       "      <td>17 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹11,63,336 /yr</td>\n",
       "      <td>₹6</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>15 salaries</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹12,18,244 /yr</td>\n",
       "      <td>₹5</td>\n",
       "      <td>₹1Cr</td>\n",
       "      <td>15 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹7,39,238 /yr</td>\n",
       "      <td>₹4</td>\n",
       "      <td>₹16L</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹13,00,000 /yr</td>\n",
       "      <td>₹8</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>13 salaries</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹8,63,750 /yr</td>\n",
       "      <td>₹5</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>10 salaries</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>EXL Service</td>\n",
       "      <td>₹11,10,000 /yr</td>\n",
       "      <td>₹6</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Optum</td>\n",
       "      <td>₹14,23,677 /yr</td>\n",
       "      <td>₹8</td>\n",
       "      <td>₹20L</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Optum Global Solutions</td>\n",
       "      <td>₹13,28,697 /yr</td>\n",
       "      <td>₹4</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Company Name  Average Salary Mini. Salary Max. Salary  \\\n",
       "0                        IBM   ₹9,00,000 /yr           ₹6        ₹27L   \n",
       "1  Tata Consultancy Services   ₹6,15,289 /yr           ₹3        ₹13L   \n",
       "2                  Accenture  ₹11,63,336 /yr           ₹6        ₹22L   \n",
       "3                  Delhivery  ₹12,18,244 /yr           ₹5        ₹1Cr   \n",
       "4         Ericsson-Worldwide   ₹7,39,238 /yr           ₹4        ₹16L   \n",
       "5         UnitedHealth Group  ₹13,00,000 /yr           ₹8        ₹15L   \n",
       "6         Valiance Solutions   ₹8,63,750 /yr           ₹5        ₹15L   \n",
       "7                EXL Service  ₹11,10,000 /yr           ₹6        ₹15L   \n",
       "8                      Optum  ₹14,23,677 /yr           ₹8        ₹20L   \n",
       "9     Optum Global Solutions  ₹13,28,697 /yr           ₹4        ₹22L   \n",
       "\n",
       "      Salarise Rating  \n",
       "0  18 salaries    3.9  \n",
       "1  17 salaries    3.9  \n",
       "2  15 salaries      4  \n",
       "3  15 salaries    3.9  \n",
       "4  14 salaries      4  \n",
       "5  13 salaries    3.6  \n",
       "6  10 salaries    4.2  \n",
       "7   9 salaries    3.6  \n",
       "8   9 salaries    3.7  \n",
       "9   9 salaries    3.9  "
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe \n",
    "df=pd.DataFrame()\n",
    "df['Company Name']=company_name[:10]\n",
    "df['Average Salary']=Avg[:10]\n",
    "df['Mini. Salary']=minimum\n",
    "df['Max. Salary']=maximum\n",
    "df['Salarise']=S\n",
    "df['Rating']=Rating\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url='https://www.flipkart.com/'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)\n",
    "\n",
    "#do click using class_name function\n",
    "cancel_btn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cancel_btn.click()\n",
    "\n",
    "search_bar=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search_bar.send_keys('Sunglasses')\n",
    "\n",
    "search_click=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data for each thing which is required from 1st  resultunt page and collect in respective list\n",
    "brand_n=[]\n",
    "PD=[]\n",
    "Price=[]\n",
    "Disc=[]\n",
    "Brand=driver.find_elements_by_class_name('_2WkVRV')\n",
    "PD1=driver.find_elements_by_class_name('IRpwTa')\n",
    "Price1=driver.find_elements_by_class_name('_30jeq3') \n",
    "Disc1=driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "for (i,j,k,l) in itertools.zip_longest(Brand,PD1,Price1,Disc1):\n",
    "    try:\n",
    "        brand_n.append(i.text)\n",
    "        PD.append(j.text)\n",
    "        Price.append(k.text)\n",
    "        Disc.append(l.text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kingsunglasses',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'PIRASO',\n",
       " 'kingsunglasses',\n",
       " 'Fastrack',\n",
       " 'PHENOMENAL',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'kingsunglasses',\n",
       " 'PHENOMENAL',\n",
       " 'Fastrack',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Silver Kartz',\n",
       " 'PIRASO',\n",
       " 'PIRASO',\n",
       " 'HIPPON',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'Fastrack',\n",
       " 'DEIXELS',\n",
       " 'PHENOMENAL',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'Fastrack',\n",
       " 'Flizz',\n",
       " 'GANSTA',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'kingsunglasses',\n",
       " 'kingsunglasses',\n",
       " 'hipe',\n",
       " 'Fastrack',\n",
       " 'Fastrack',\n",
       " 'ROZZETTA CRAFT',\n",
       " 'hipe',\n",
       " 'kingsunglasses',\n",
       " 'PIRASO',\n",
       " 'NuVew',\n",
       " 'ROZZETTA CRAFT']"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.DataFrame()\n",
    "# df['Brand Name']=brand_n\n",
    "# df['Product Dis.']=PD\n",
    "# df['Price']=Pri- ce\n",
    "# df['Discount']=Disc\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,3,1):\n",
    "#     nxt_button=driver.find_element_by_xpath('/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "#     nxt_button.click()\n",
    "#     Brand=driver.find_elements_by_class_name('_2WkVRV')\n",
    "#     PD1=driver.find_elements_by_class_name('IRpwTa')\n",
    "#     Price1=driver.find_elements_by_class_name('_30jeq3')\n",
    "#     Disc1=driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "#     for j,k,l,m in itertools.zip_longest(Brand,PD1,Price1,Disc1):\n",
    "#         if len(brand_n)>99:\n",
    "#             break\n",
    "#         else:\n",
    "#             try:\n",
    "#                 brand_n.append(j.text)\n",
    "#                 PD.append(k.text)\n",
    "#                 Price.append(l.text)\n",
    "#                 Disc.append(m.text)\n",
    "#             except:\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls=[]\n",
    "nxt=driver.find_elements_by_xpath('//nav[@class=\"yFHi8N\"]//a')\n",
    "for i in nxt:\n",
    "    ls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #collect all the links in list and refer it as per requirement\n",
    "# nxt_pg=[]\n",
    "# url=driver.find_elements_by_xpath('//div[@class=\"_2MImiq\"]//a')\n",
    "# for i in url:\n",
    "#     nxt_pg.append(i.get_attribute(\"href\"))\n",
    "# nxt_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect data from next page till it reach 100 count.\n",
    "#Collect data from next page , which refer by link \n",
    "for i in ls[1::]:\n",
    "    driver.get(i)\n",
    "    Brand=driver.find_elements_by_class_name('_2WkVRV')\n",
    "    PD1=driver.find_elements_by_class_name('IRpwTa')\n",
    "    Price1=driver.find_elements_by_class_name('_30jeq3')\n",
    "    Disc1=driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "    for j,k,l,m in itertools.zip_longest(Brand,PD1,Price1,Disc1):\n",
    "        if len(brand_n)==100:\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                brand_n.append(j.text)\n",
    "                PD.append(k.text)\n",
    "                Price.append(l.text)\n",
    "                Disc.append(m.text)\n",
    "            except:\n",
    "                pass            \n",
    "    if len(brand_n)==100:\n",
    "        break\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100, 100)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand_n),len(PD),len(Price),len(Disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Dis.</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹299</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹513</td>\n",
       "      <td>₹286 off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹758</td>\n",
       "      <td>₹141 off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹237</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹349</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Wrogn</td>\n",
       "      <td>Mirrored Wayfarer Sunglasses (51)</td>\n",
       "      <td>₹663</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>DEIXELS</td>\n",
       "      <td>Polarized, UV Protection, Riding Glasses Wayfa...</td>\n",
       "      <td>₹249</td>\n",
       "      <td>58% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹315</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Oval Sunglasses (56)</td>\n",
       "      <td>₹299</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Fravy</td>\n",
       "      <td>UV Protection, Gradient, Night Vision Retro Sq...</td>\n",
       "      <td>₹339</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand Name                                       Product Dis. Price  \\\n",
       "0   kingsunglasses  Mirrored, UV Protection Wayfarer Sunglasses (F...  ₹299   \n",
       "1         Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹513   \n",
       "2         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ₹758   \n",
       "3           PIRASO              UV Protection Aviator Sunglasses (54)  ₹237   \n",
       "4   kingsunglasses         UV Protection Round Sunglasses (Free Size)  ₹349   \n",
       "..             ...                                                ...   ...   \n",
       "95           Wrogn                  Mirrored Wayfarer Sunglasses (51)  ₹663   \n",
       "96         DEIXELS  Polarized, UV Protection, Riding Glasses Wayfa...  ₹249   \n",
       "97  kingsunglasses         UV Protection Round Sunglasses (Free Size)  ₹315   \n",
       "98    Silver Kartz                 UV Protection Oval Sunglasses (56)  ₹299   \n",
       "99           Fravy  UV Protection, Gradient, Night Vision Retro Sq...  ₹339   \n",
       "\n",
       "    Discount  \n",
       "0    88% off  \n",
       "1   ₹286 off  \n",
       "2   ₹141 off  \n",
       "3    85% off  \n",
       "4    78% off  \n",
       "..       ...  \n",
       "95   73% off  \n",
       "96   58% off  \n",
       "97   81% off  \n",
       "98   75% off  \n",
       "99   83% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({})\n",
    "df['Brand Name']=brand_n\n",
    "df['Product Dis.']=PD\n",
    "df['Price']=Price\n",
    "df['Discount']=Disc\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on\"show all reviews\"\n",
    "all_rev=driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div/div/div[5]/div/a')\n",
    "all_rev.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review_summary=[]\n",
    "Full_review=[]\n",
    "Rating=[]\n",
    "RS=driver.find_elements_by_class_name('_2-N8zT')\n",
    "FR=driver.find_elements_by_class_name('t-ZTKy')\n",
    "R=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for (i,j,k) in itertools.zip_longest(RS,FR,R):\n",
    "    try:\n",
    "        Review_summary.append(i.text)\n",
    "        Full_review.append(j.text.replace('\\n',''))\n",
    "        Rating.append(k.text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '5', '5', '5', '5', '5', '5', '4', '5', '5']"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #send get request to the webpage server to get the source code of the page\n",
    "# url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\"\n",
    "# page = requests.get(url)\n",
    "# #see content in page\n",
    "# soup = BeautifulSoup(page.content)\n",
    "# rating = []   #empty list\n",
    "\n",
    "# # Scrape product rating\n",
    "# for i in soup.find_all(\"div\",class_=\"_3LWZlK _1BLPMq\"):\n",
    "#     rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Review_summary),len(Full_review),len(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all the links in list and refer it as per requirement\n",
    "nxt_pg=[]\n",
    "url=driver.find_elements_by_xpath('//nav[@class=\"yFHi8N\"]//a')\n",
    "for i in url:\n",
    "    nxt_pg.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=1',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=2',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=3',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=4',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=5',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=6',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=7',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=8',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=9',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=10',\n",
       " 'https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=2']"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxt_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in nxt_pg[1:]:\n",
    "#     driver.get(i)\n",
    "#     RS=driver.find_elements_by_class_name('_2-N8zT')\n",
    "#     FR=driver.find_elements_by_class_name('t-ZTKy')\n",
    "#     #send get request to the webpage server to get the source code of the page\n",
    "#     page = requests.get(i)\n",
    "#     #see content in page\n",
    "#     soup = BeautifulSoup(page.content)\n",
    "#     if len(Review_summary)>99:\n",
    "#             break\n",
    "#     else:\n",
    "#         # Scrape product rating\n",
    "#         for i in soup.find_all(\"div\",class_=\"_3LWZlK _1BLPMq\"):\n",
    "#             Rating.append(i.text)\n",
    "#         for (i,j) in zip(RS,FR):\n",
    "#                 Review_summary.append(i.text)\n",
    "#                 Full_review.append(j.text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nxt_pg[1:]:\n",
    "    driver.get(i)\n",
    "    RS=driver.find_elements_by_class_name('_2-N8zT')\n",
    "    FR=driver.find_elements_by_class_name('t-ZTKy')\n",
    "    R=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for (i,j,k) in itertools.zip_longest(RS,FR,R):\n",
    "        try:\n",
    "            Review_summary.append(i.text)\n",
    "            Full_review.append(j.text.replace('\\n',''))\n",
    "            Rating.append(k.text)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 110, 108)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Review_summary),len(Full_review),len(Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_summary</th>\n",
       "      <th>Full_review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the MoneyThe iPhone 11 offe...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>The best all rounder iphone. Flipkart is doing...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>A wort full value for money decision it’s . Si...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>I genuinely liked it. One of the best mobile p...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>If you are looking for a premium phone under 5...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Good quality product</td>\n",
       "      <td>Awesome camera, smooth and fast UI, display is...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Review_summary                                        Full_review  \\\n",
       "0              Brilliant  The Best Phone for the MoneyThe iPhone 11 offe...   \n",
       "1         Simply awesome  Really satisfied with the Product I received.....   \n",
       "2       Perfect product!  Amazing phone with great cameras and better ba...   \n",
       "3              Fabulous!  This is my first iOS phone. I am very happy wi...   \n",
       "4      Worth every penny  Previously I was using one plus 3t it was a gr...   \n",
       "..                   ...                                                ...   \n",
       "95   Best in the market!  The best all rounder iphone. Flipkart is doing...   \n",
       "96             Brilliant  A wort full value for money decision it’s . Si...   \n",
       "97             Wonderful  I genuinely liked it. One of the best mobile p...   \n",
       "98          Nice product  If you are looking for a premium phone under 5...   \n",
       "99  Good quality product  Awesome camera, smooth and fast UI, display is...   \n",
       "\n",
       "   Rating  \n",
       "0       5  \n",
       "1       5  \n",
       "2       5  \n",
       "3       5  \n",
       "4       5  \n",
       "..    ...  \n",
       "95      4  \n",
       "96      4  \n",
       "97      4  \n",
       "98      5  \n",
       "99      5  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({})\n",
    "df['Review_summary']=Review_summary[:100]\n",
    "df['Full_review']=Full_review[:100]\n",
    "df['Rating']=Rating[:100]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url='https://www.flipkart.com/'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "#do click using class_name function\n",
    "cancel_btn=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "cancel_btn.click()\n",
    "\n",
    "#enter on search bar \n",
    "search_bar=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')\n",
    "search_bar.send_keys('sneakers')\n",
    "\n",
    "#do click on search button\n",
    "search_click=driver.find_element_by_xpath('/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_click.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data for each thing which is required from 1st  resultunt page and collect in respective list\n",
    "brand=[]\n",
    "PD=[]\n",
    "Price=[]\n",
    "Disc=[]\n",
    "Brand=driver.find_elements_by_class_name('_2WkVRV')\n",
    "Pd=driver.find_elements_by_class_name('IRpwTa')\n",
    "Price1=driver.find_elements_by_class_name('_30jeq3') \n",
    "Disc1=driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "for (i,j,k,l) in itertools.zip_longest(Brand,Pd,Price1,Disc1):\n",
    "    try:\n",
    "        brand.append(i.text)\n",
    "        PD.append(j.text)\n",
    "        Price.append(k.text)\n",
    "        Disc.append(l.text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all the links in list and refer it as per requirement\n",
    "nxt_pg=[]\n",
    "url=driver.find_elements_by_xpath('//nav[@class=\"yFHi8N\"]//a')\n",
    "for i in url:\n",
    "    nxt_pg.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 40, 40)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand),len(PD),len(Price),len(Disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nxt_pg[1:3:]:\n",
    "        driver.get(i)\n",
    "        Brand=driver.find_elements_by_class_name('_2WkVRV')\n",
    "        Pd=driver.find_elements_by_class_name('IRpwTa')\n",
    "        Price1=driver.find_elements_by_class_name('_30jeq3') \n",
    "        Disc1=driver.find_elements_by_class_name('_3Ay6Sb')\n",
    "        for (i,j,k,l) in itertools.zip_longest(Brand,Pd,Price1,Disc1):\n",
    "            if len(brand)==100:\n",
    "                        break\n",
    "            else:       \n",
    "                    try:  \n",
    "                        brand.append(i.text)\n",
    "                        PD.append(j.text)\n",
    "                        Price.append(k.text)\n",
    "                        Disc.append(l.text)\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100, 100)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand),len(PD),len(Price),len(Disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Dis.</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>DUCATI</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹1,445</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Nilatin</td>\n",
       "      <td>White Sneaker For Men Sneakers For Men</td>\n",
       "      <td>₹409</td>\n",
       "      <td>68% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CALCADOS</td>\n",
       "      <td>Modern Trendy Shoes Combo pack of 4 Sneakers F...</td>\n",
       "      <td>₹748</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>French Connection</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹1,374</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>ESSENCE</td>\n",
       "      <td>Trendy Sneakers For Men</td>\n",
       "      <td>₹468</td>\n",
       "      <td>53% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>SPARX</td>\n",
       "      <td>SM-162 Sneakers For Men</td>\n",
       "      <td>₹749</td>\n",
       "      <td>21% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>Fzzirok</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Bonexy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand Name                                       Product Dis.  \\\n",
       "0              DUCATI                                   Sneakers For Men   \n",
       "1             Nilatin             White Sneaker For Men Sneakers For Men   \n",
       "2        Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "3            CALCADOS  Modern Trendy Shoes Combo pack of 4 Sneakers F...   \n",
       "4              Chevit  Super Stylish & Trendy Combo Pack of 02 Pairs ...   \n",
       "..                ...                                                ...   \n",
       "95  French Connection                                   Sneakers For Men   \n",
       "96            ESSENCE                            Trendy Sneakers For Men   \n",
       "97              SPARX                            SM-162 Sneakers For Men   \n",
       "98            Fzzirok                                   Sneakers For Men   \n",
       "99             Bonexy                                   Sneakers For Men   \n",
       "\n",
       "     Price Discount  \n",
       "0   ₹1,445  60% off  \n",
       "1     ₹409  68% off  \n",
       "2     ₹399  60% off  \n",
       "3     ₹748  62% off  \n",
       "4     ₹599  62% off  \n",
       "..     ...      ...  \n",
       "95  ₹1,374  62% off  \n",
       "96    ₹468  53% off  \n",
       "97    ₹749  21% off  \n",
       "98    ₹399  60% off  \n",
       "99    ₹449  55% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({})\n",
    "df['Brand Name']=brand\n",
    "df['Product Dis.']=PD\n",
    "df['Price']=Price\n",
    "df['Discount']=Disc\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url='https://www.myntra.com/shoes'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select color\n",
    "Color=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label')\n",
    "Color.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select Price\n",
    "Price=driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label')\n",
    "Price.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=[]\n",
    "SSD=[]\n",
    "Price=[]\n",
    "def repeat():\n",
    "    br=driver.find_elements_by_class_name('product-brand')\n",
    "    ssd=driver.find_elements_by_class_name('product-product')\n",
    "    price=driver.find_elements_by_class_name('product-price')\n",
    "    for (i,j,k) in itertools.zip_longest(br,ssd,price):\n",
    "        try:\n",
    "            Brand.append(i.text)\n",
    "            SSD.append(j.text)\n",
    "            Price.append(k.text)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 50)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat()\n",
    "len(Brand),len(SSD),len(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on next page \n",
    "nxt_page=driver.find_element_by_xpath('//*[@id=\"desktopSearchResults\"]/div[2]/section/div[2]/ul/li[12]/a')\n",
    "nxt_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Brand),len(SSD),len(Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Short Dis.</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Nike</td>\n",
       "      <td>LEBRON XVIII Basketball Shoes</td>\n",
       "      <td>Rs. 12316Rs. 17595(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Nike</td>\n",
       "      <td>Men AIR ZOOM Running Shoes</td>\n",
       "      <td>Rs. 7721Rs. 10295(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Nike</td>\n",
       "      <td>Unisex LEBRON XVIII Basketball</td>\n",
       "      <td>Rs. 12316Rs. 17595(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Driving Shoes</td>\n",
       "      <td>Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Leather Pumps</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>VIONIC</td>\n",
       "      <td>Men Textured Sneakers</td>\n",
       "      <td>Rs. 7039Rs. 10999(36% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>J.FONTINI</td>\n",
       "      <td>Men Textured Leather Loafers</td>\n",
       "      <td>Rs. 7690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Textured Oxfords</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Woven Design Sneakers</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brand Name                      Short Dis.                        Price\n",
       "0           Nike   LEBRON XVIII Basketball Shoes  Rs. 12316Rs. 17595(30% OFF)\n",
       "1           Nike      Men AIR ZOOM Running Shoes   Rs. 7721Rs. 10295(25% OFF)\n",
       "2           ALDO                    Men Sneakers                     Rs. 9999\n",
       "3           Nike  Unisex LEBRON XVIII Basketball  Rs. 12316Rs. 17595(30% OFF)\n",
       "4           ALDO               Men Driving Shoes                    Rs. 11999\n",
       "..           ...                             ...                          ...\n",
       "95          Geox             Women Leather Pumps                     Rs. 9999\n",
       "96        VIONIC           Men Textured Sneakers   Rs. 7039Rs. 10999(36% OFF)\n",
       "97     J.FONTINI    Men Textured Leather Loafers                     Rs. 7690\n",
       "98  Hush Puppies            Men Textured Oxfords                     Rs. 6999\n",
       "99  Hush Puppies       Men Woven Design Sneakers                     Rs. 6999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({})\n",
    "df['Brand Name']=Brand\n",
    "df['Short Dis.']=SSD\n",
    "df['Price']=Price\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: Go to webpage https://www.amazon.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#CONNECT TO WEB-DRIVER\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\Ashish\\Downloads\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "# Step 1 :- 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "url=' https://www.amazon.in/'\n",
    "#open the site in our automatic chrome browser\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar=driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')\n",
    "search_bar.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]')\n",
    "search_btn.click() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on brand\n",
    "typ=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[26]/span/a')\n",
    "typ.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on brand\n",
    "typ=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[28]/span/a')\n",
    "typ.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]\n",
    "tit=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "prs=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for (i,k) in itertools.zip_longest(tit,prs):\n",
    "    if len(Title)==7:\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            Title.append(i.text)\n",
    "            Price.append(k.text)\n",
    "        except:\n",
    "            pass               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send get request to the webpage server to get the source code of the page\n",
    "url = \"https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1626037299&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_27\"\n",
    "page = requests.get(url)\n",
    "#see content in page\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "# Scrape product rating\n",
    "for i in soup.find_all(\"span\",class_=\"a-icon-alt\"):\n",
    "    Rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.4 out of 5 stars',\n",
       " '4.0 out of 5 stars',\n",
       " '4.4 out of 5 stars',\n",
       " '4.6 out of 5 stars',\n",
       " '4.0 out of 5 stars',\n",
       " '3.8 out of 5 stars',\n",
       " '4.6 out of 5 stars',\n",
       " '4.4 out of 5 stars',\n",
       " '2.9 out of 5 stars',\n",
       " '4.3 out of 5 stars',\n",
       " '1.0 out of 5 stars',\n",
       " '4.0 out of 5 stars',\n",
       " '4.2 out of 5 stars',\n",
       " '1.9 out of 5 stars',\n",
       " '4.2 out of 5 stars',\n",
       " '1.0 out of 5 stars',\n",
       " '4 Stars & Up',\n",
       " '3 Stars & Up',\n",
       " '2 Stars & Up',\n",
       " '1 Star & Up']"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i5-1...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>54,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Lenovo Legion 7 10th Gen Intel Core i7 15.6 in...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>2,00,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HP Pavilion (2021) Thin &amp; Light 11th Gen Core ...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i7-1...</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>59,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>HP 14 Thin &amp; Light 14\" (35.56cms) FHD Laptop (...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>88,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...</td>\n",
       "      <td>3.8 out of 5 stars</td>\n",
       "      <td>78,993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(Renewed) Dell Intel Core i7 4th Gen 14 Inch(3...</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>49,999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title              Rating  \\\n",
       "0  Mi Notebook Horizon Edition 14 Intel Core i5-1...  4.4 out of 5 stars   \n",
       "1  Lenovo Legion 7 10th Gen Intel Core i7 15.6 in...  4.0 out of 5 stars   \n",
       "2  HP Pavilion (2021) Thin & Light 11th Gen Core ...  4.4 out of 5 stars   \n",
       "3  Mi Notebook Horizon Edition 14 Intel Core i7-1...  4.6 out of 5 stars   \n",
       "4  HP 14 Thin & Light 14\" (35.56cms) FHD Laptop (...  4.0 out of 5 stars   \n",
       "5  HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...  3.8 out of 5 stars   \n",
       "6  (Renewed) Dell Intel Core i7 4th Gen 14 Inch(3...  4.6 out of 5 stars   \n",
       "\n",
       "      Price  \n",
       "0    54,999  \n",
       "1  2,00,000  \n",
       "2    84,990  \n",
       "3    59,999  \n",
       "4    88,990  \n",
       "5    78,993  \n",
       "6    49,999  "
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({})\n",
    "df['Title']=Title\n",
    "df['Rating']=Rating[:7]\n",
    "df['Price']=Price\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
